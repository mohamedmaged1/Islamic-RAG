{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import DashScopeEmbeddings  \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dashscope_api_key = \"<put_your_dashscope_api_key_here>\"\n",
    "dashscope_base_url = \"<put_your_dashscope_base_url_here>\"\n",
    "\n",
    "\n",
    "with open(\"./Data/all_splits.pkl\", \"rb\") as f:\n",
    "    all_splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05998fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build your own system prompt in Arabic or adjusted tone\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",  \" أنت عالم فقه مسلم فى غايه الذكاء مقدم لك بيانات لتستمد منها اجاباتك بناء على سؤال مقدم لك واعلم انه يوجد ايات قرءانية مكتوبه بطريقه خطأ لا تستمد منها أيضا تجنب الحروف غير العربية\"),\n",
    "    (\"human\", \"استخدم السياق التالي للإجابة على السؤال:\\n\\n{context}\\n\\nالسؤال: {question}\\n\\nالإجابة:\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    dashscope_api_key=dashscope_api_key,\n",
    "    model=\"text-embedding-v1\"  \n",
    ")\n",
    "\n",
    "# Create vectorstore\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "retriever = vector_store.as_retriever(   \n",
    "    search_type=\"similarity\",    # or \"mmr\", \"similarity_score_threshold\"\n",
    "    search_kwargs={\"k\": 3}       # how many results to retrieve\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LLM for DashScope\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"qwen3-8b\", \n",
    "    openai_api_key=dashscope_api_key,\n",
    "    openai_api_base=dashscope_base_url,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60021f8d",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "Data = pd.read_csv(\".\\evaluation_dataset_plus_164.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85a05a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   1%|          | 1/163 [00:55<2:28:56, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   3%|▎         | 5/163 [04:40<2:15:03, 51.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   9%|▉         | 15/163 [11:12<1:32:23, 37.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  11%|█         | 18/163 [14:04<2:00:25, 49.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  15%|█▌        | 25/163 [18:36<1:49:56, 47.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  18%|█▊        | 29/163 [21:56<1:45:45, 47.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  27%|██▋       | 44/163 [33:31<1:19:13, 39.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  29%|██▉       | 48/163 [38:00<2:14:07, 69.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  37%|███▋      | 61/163 [47:32<1:26:38, 50.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  38%|███▊      | 62/163 [49:14<1:51:50, 66.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  42%|████▏     | 69/163 [54:29<1:06:32, 42.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 68: Error code: 400 - {'error': {'code': 'data_inspection_failed', 'param': None, 'message': 'Input data may contain inappropriate content.', 'type': 'data_inspection_failed'}, 'id': 'chatcmpl-1e9a90b0-f2ba-958c-bb8d-94fef518e33f', 'request_id': '1e9a90b0-f2ba-958c-bb8d-94fef518e33f'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  44%|████▎     | 71/163 [57:21<1:39:10, 64.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  48%|████▊     | 79/163 [1:02:53<1:06:46, 47.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  52%|█████▏    | 84/163 [2:00:27<20:36:06, 938.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 83: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  52%|█████▏    | 85/163 [2:00:28<14:14:48, 657.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 84: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  53%|█████▎    | 86/163 [2:00:29<9:51:12, 460.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 85: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  53%|█████▎    | 87/163 [2:00:30<6:48:59, 322.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 86: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  54%|█████▍    | 88/163 [2:00:32<4:43:01, 226.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 87: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  55%|█████▍    | 89/163 [2:00:33<3:15:56, 158.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 88: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  55%|█████▌    | 90/163 [2:00:34<2:15:49, 111.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 89: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  56%|█████▌    | 91/163 [2:00:36<1:34:13, 78.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both attempts failed for question 90: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  56%|█████▋    | 92/163 [2:02:01<1:35:09, 80.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  58%|█████▊    | 95/163 [2:03:44<56:39, 50.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  63%|██████▎   | 102/163 [2:08:47<38:29, 37.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  68%|██████▊   | 111/163 [2:13:13<28:27, 32.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  85%|████████▍ | 138/163 [2:30:34<14:56, 35.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  88%|████████▊ | 143/163 [2:34:05<14:48, 44.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  93%|█████████▎| 152/163 [2:40:31<09:45, 53.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got fallback response for question 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 163/163 [2:47:27<00:00, 61.64s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"deepseek\"\n",
    "Data[model] = ''\n",
    "for i in tqdm(range(Data.shape[0]), desc=\"Processing questions\"):\n",
    "# Re-run the specific questions that had errors\n",
    "    question = Data['question'][i]\n",
    "    try:\n",
    "        Data.loc[i, model] = rag_chain.invoke(question)\n",
    "    except :\n",
    "        try:\n",
    "            # Fallback: Try getting response without context\n",
    "            prompt_without_context = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \" أنت عالم فقه مسلم فى غايه الذكاء\"),\n",
    "                (\"human\", \"{question}\")\n",
    "            ])\n",
    "            chain_without_context = prompt_without_context | llm | StrOutputParser()\n",
    "            out = chain_without_context.invoke({\"question\": question})\n",
    "            Data.loc[i, model] = out\n",
    "            print(f\"Got fallback response for question {i}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Both attempts failed for question {i}: {e2}\")\n",
    "            Data.loc[i, model] = \"No Response\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d617b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yalla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
